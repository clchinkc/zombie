import numpy as np
import tensorflow as tf
from keras.layers import Dense, Flatten, Input, LeakyReLU, Reshape
from keras.models import Model, Sequential
from keras.optimizers import Adam


# Define the generator
def build_generator(latent_dim, data_shape):
    model = Sequential()
    model.add(Input(shape=(latent_dim,)))
    model.add(Dense(128, activation=LeakyReLU(negative_slope=0.2)))
    model.add(Dense(np.prod(data_shape), activation='tanh'))
    model.add(Reshape(data_shape))
    return model

# Define the discriminator
def build_discriminator(data_shape):
    model = Sequential()
    model.add(Input(shape=data_shape))
    model.add(Flatten())
    model.add(Dense(128, activation=LeakyReLU(negative_slope=0.2)))
    model.add(Dense(1, activation='sigmoid'))
    return model

# Define GAN
def build_gan(generator, discriminator):
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    discriminator.trainable = False
    gan_input = Input(shape=(latent_dim,))
    gan_output = discriminator(generator(gan_input))
    gan = Model(gan_input, gan_output)
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    return gan

# GAN training function
def train_gan(gan, generator, discriminator, latent_dim, epochs, batch_size, data_shape):
    valid = np.ones((batch_size, 1))
    fake = np.zeros((batch_size, 1))

    for epoch in range(epochs):
        # Generate random noise as input
        noise = np.random.normal(0, 1, (batch_size, latent_dim))

        # Generate fake images
        gen_imgs = generator.predict(noise)

        # Generate real images
        real_imgs = np.random.normal(0, 1, (batch_size, *data_shape))

        # Train the discriminator (real classified as ones and generated as zeros)
        d_loss_real = discriminator.train_on_batch(real_imgs, valid)
        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator (wants discriminator to mistake images as real)
        g_loss = gan.train_on_batch(noise, valid)

        # Print progress
        print(f"Epoch: {epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]")


# Set dimensions and shapes
latent_dim = 100
data_shape = (10, 10)  # Shape of the data to be generated by the GAN

# Build and compile GAN
generator = build_generator(latent_dim, data_shape)
discriminator = build_discriminator(data_shape)
gan = build_gan(generator, discriminator)

# Train GAN
train_gan(gan, generator, discriminator, latent_dim, epochs=10, batch_size=16, data_shape=data_shape)
